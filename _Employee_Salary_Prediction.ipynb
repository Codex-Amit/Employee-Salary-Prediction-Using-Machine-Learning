{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab4e36-a21b-4d8e-8e5a-5334c5791056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee Salary Prediction - Advanced ML & Deep Learning Analysis\n",
    "# This notebook demonstrates comprehensive salary prediction using various AI/ML techniques\n",
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9ced7-c576-4169-b569-c7cd5c468950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Explore Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset from uploaded file\n",
    "data = pd.read_csv('adult.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c818500-fb38-456c-a2f1-aa598615d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization and Analysis\n",
    "def visualize_data(data):\n",
    "    \"\"\"Create comprehensive visualizations of the dataset\"\"\"\n",
    "    \n",
    "    # Set up the plotting area\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Age distribution\n",
    "    axes[0,0].hist(data['age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('Age Distribution')\n",
    "    axes[0,0].set_xlabel('Age')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Income distribution\n",
    "    income_counts = data['income'].value_counts()\n",
    "    axes[0,1].pie(income_counts.values, labels=income_counts.index, autopct='%1.1f%%', \n",
    "                  colors=['lightcoral', 'lightblue'])\n",
    "    axes[0,1].set_title('Income Distribution')\n",
    "    \n",
    "    # Education levels\n",
    "    education_counts = data['education'].value_counts()\n",
    "    axes[0,2].barh(education_counts.index[:10], education_counts.values[:10])\n",
    "    axes[0,2].set_title('Top 10 Education Levels')\n",
    "    axes[0,2].set_xlabel('Count')\n",
    "    \n",
    "    # Workclass distribution\n",
    "    workclass_counts = data['workclass'].value_counts()\n",
    "    axes[1,0].bar(workclass_counts.index[:8], workclass_counts.values[:8], \n",
    "                  color='lightgreen', alpha=0.7)\n",
    "    axes[1,0].set_title('Workclass Distribution')\n",
    "    axes[1,0].set_xlabel('Workclass')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hours per week vs Income\n",
    "    income_groups = data.groupby('income')['hours-per-week'].mean()\n",
    "    axes[1,1].bar(income_groups.index, income_groups.values, color=['orange', 'purple'])\n",
    "    axes[1,1].set_title('Average Hours per Week by Income')\n",
    "    axes[1,1].set_xlabel('Income Category')\n",
    "    axes[1,1].set_ylabel('Average Hours/Week')\n",
    "    \n",
    "    # Gender and Income correlation\n",
    "    gender_income = pd.crosstab(data['gender'], data['income'])\n",
    "    gender_income.plot(kind='bar', ax=axes[1,2], color=['pink', 'lightblue'])\n",
    "    axes[1,2].set_title('Gender vs Income')\n",
    "    axes[1,2].set_xlabel('Gender')\n",
    "    axes[1,2].set_ylabel('Count')\n",
    "    axes[1,2].legend(['<=50K', '>50K'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation heatmap for numerical features\n",
    "    numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    if all(col in data.columns for col in numerical_cols):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = data[numerical_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, fmt='.2f')\n",
    "        plt.title('Correlation Matrix of Numerical Features')\n",
    "        plt.show()\n",
    "\n",
    "visualize_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa384b0-d2ff-4085-b3ee-ad7310677469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Comprehensive data preprocessing pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    processed_data.replace('?', np.nan, inplace=True)\n",
    "    \n",
    "    null_counts = processed_data.isnull().sum()\n",
    "    print(f\"Null values per column:\\n{null_counts[null_counts > 0]}\")\n",
    "    \n",
    "    for col in processed_data.columns:\n",
    "        if processed_data[col].dtype == 'object' and processed_data[col].isnull().sum() > 0:\n",
    "            mode_val = processed_data[col].mode()[0]\n",
    "            processed_data[col].fillna(mode_val, inplace=True)\n",
    "    \n",
    "    processed_data = processed_data[~processed_data['workclass'].isin(['Never-worked', 'Without-pay'])]\n",
    "    \n",
    "    processed_data = processed_data[~processed_data['education'].isin(['Preschool', '1st-4th', '5th-6th'])]\n",
    "    \n",
    "    Q1 = processed_data['age'].quantile(0.25)\n",
    "    Q3 = processed_data['age'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    processed_data = processed_data[(processed_data['age'] >= max(17, lower_bound)) & \n",
    "                                   (processed_data['age'] <= min(75, upper_bound))]\n",
    "    \n",
    "    print(f\"Data shape after preprocessing: {processed_data.shape}\")\n",
    "    return processed_data\n",
    "\n",
    "processed_data = preprocess_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395117dc-0183-4deb-a99a-6076f3d723bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Encoding\n",
    "def feature_engineering(data):\n",
    "    \"\"\"Advanced feature engineering and encoding\"\"\"\n",
    "    \n",
    "    print(\"Starting feature engineering...\")\n",
    "    \n",
    "    data['age_group'] = pd.cut(data['age'], bins=[0, 25, 35, 50, 65, 100], \n",
    "                              labels=['Young', 'Adult', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "    \n",
    "    data['hours_category'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], \n",
    "                                   labels=['Part-time', 'Full-time', 'Overtime', 'Workaholic'])\n",
    "    \n",
    "    data['capital_net'] = data['capital-gain'] - data['capital-loss']\n",
    "    \n",
    "    data['has_capital_gain'] = (data['capital-gain'] > 0).astype(int)\n",
    "    data['has_capital_loss'] = (data['capital-loss'] > 0).astype(int)\n",
    "    \n",
    "    education_mapping = {\n",
    "        'Doctorate': 'Advanced',\n",
    "        'Prof-school': 'Advanced',\n",
    "        'Masters': 'Advanced',\n",
    "        'Bachelors': 'Bachelors',\n",
    "        'Some-college': 'Some-college',\n",
    "        'Assoc-acdm': 'Associate',\n",
    "        'Assoc-voc': 'Associate',\n",
    "        'HS-grad': 'High-school',\n",
    "        '12th': 'High-school',\n",
    "        '11th': 'High-school',\n",
    "        '10th': 'High-school',\n",
    "        '9th': 'High-school',\n",
    "        '7th-8th': 'Elementary',\n",
    "    }\n",
    "    data['education_grouped'] = data['education'].map(education_mapping).fillna('Other')\n",
    "    \n",
    "    columns_to_drop = ['education', 'fnlwgt']  # fnlwgt is a sampling weight, not useful for prediction\n",
    "    data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "    \n",
    "    print(f\"Feature engineering completed. New shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "engineered_data = feature_engineering(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d87af0-039b-418a-a999-b32b76be6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c0aab-2f16-49e7-9522-562e7e1efcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Label Encoding and Normalization\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "def encode_and_normalize(data):\n",
    "    X = data.drop(\"income\", axis=1)\n",
    "    y = data[\"income\"]\n",
    "\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    target_encoder = LabelEncoder()\n",
    "    y = target_encoder.fit_transform(y)\n",
    "\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    scaler_standard = StandardScaler()\n",
    "\n",
    "    X_minmax = scaler_minmax.fit_transform(X)\n",
    "    X_standard = scaler_standard.fit_transform(X)\n",
    "\n",
    "    print(f\"Encoded features shape: {X.shape}\")\n",
    "    return X, y, X_minmax, X_standard, label_encoders, target_encoder, scaler_minmax, scaler_standard\n",
    "\n",
    "X, y, X_minmax, X_standard, label_encoders, target_encoder, scaler_minmax, scaler_standard = encode_and_normalize(engineered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79430920-6d48-4f07-9582-d09fd567ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split with Stratification\n",
    "def create_train_test_split(X_minmax, X_standard, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Create stratified train-test splits for both scaling methods\"\"\"\n",
    "    \n",
    "    X_train_mm, X_test_mm, y_train_mm, y_test_mm = train_test_split(\n",
    "        X_minmax, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train_std, X_test_std, y_train_std, y_test_std = train_test_split(\n",
    "        X_standard, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train_mm.shape}\")\n",
    "    print(f\"Test set shape: {X_test_mm.shape}\")\n",
    "    print(f\"Class distribution in training: {np.bincount(y_train_mm)}\")\n",
    "    print(f\"Class distribution in test: {np.bincount(y_test_mm)}\")\n",
    "    \n",
    "    return X_train_mm, X_test_mm, y_train_mm, y_test_mm, X_train_std, X_test_std, y_train_std, y_test_std\n",
    "\n",
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm, X_train_std, X_test_std, y_train_std, y_test_std = create_train_test_split(X_minmax, X_standard, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52df02-b52c-4e42-86e1-3b0be3eaa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Machine Learning Models with Hyperparameter Tuning\n",
    "def train_traditional_models(X_train, X_test, y_train, y_test, scaling_method=\"MinMax\"):\n",
    "    \"\"\"Train and evaluate traditional ML models with hyperparameter tuning\"\"\"\n",
    "    \n",
    "    print(f\"Training traditional ML models with {scaling_method} scaling...\")\n",
    "    \n",
    "    models = {\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'SVM': SVC(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_grids = {\n",
    "        'KNN': {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']},\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']},\n",
    "        'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None]},\n",
    "        'SVM': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2], 'max_depth': [3, 5]}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': best_model,\n",
    "            'accuracy': accuracy,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180b448-41ed-4326-a600-04f3e6114310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Training with MinMax Scaling ===\")\n",
    "results_minmax = train_traditional_models(X_train_mm, X_test_mm, y_train_mm, y_test_mm, \"MinMax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0747c3-0536-4ee7-83b0-21ddefcfca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training with Standard Scaling ===\")\n",
    "results_standard = train_traditional_models(X_train_std, X_test_std, y_train_std, y_test_std, \"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084366f-c9d0-4bb5-97b9-d394fc2163f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Model with TensorFlow/Keras\n",
    "def create_deep_learning_model(input_dim, hidden_layers=[128, 64, 32], dropout_rate=0.3):\n",
    "    \"\"\"Create a deep neural network for salary prediction\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_deep_learning_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate deep learning model\"\"\"\n",
    "    \n",
    "    print(\"Training Deep Learning Model...\")\n",
    "    \n",
    "    model = create_deep_learning_model(X_train.shape[1])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       epochs=100,\n",
    "                       batch_size=32,\n",
    "                       callbacks=[early_stopping],\n",
    "                       verbose=1)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    print(f\"Deep Learning Model - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return model, history, y_pred, accuracy\n",
    "\n",
    "# Train deep learning model\n",
    "dl_model, dl_history, dl_predictions, dl_accuracy = train_deep_learning_model(X_train_std, X_test_std, y_train_std, y_test_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d792c3-be16-4c2e-b45a-fc84b0f3a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Comparison Between MinMax and Standard Scaling\n",
    "def plot_scaling_comparison(results_minmax, results_standard, dl_accuracy):\n",
    "    \"\"\"Plot model accuracy comparison for MinMax vs Standard scaling\"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Collect accuracies\n",
    "    model_names = []\n",
    "    minmax_accuracies = []\n",
    "    standard_accuracies = []\n",
    "\n",
    "    for name in results_minmax.keys():\n",
    "        model_names.append(name)\n",
    "        minmax_accuracies.append(results_minmax[name]['accuracy'])\n",
    "        standard_accuracies.append(results_standard[name]['accuracy'])\n",
    "\n",
    "    # Add deep learning\n",
    "    model_names.append('Deep Learning')\n",
    "    minmax_accuracies.append(dl_accuracy)\n",
    "    standard_accuracies.append(dl_accuracy)\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, minmax_accuracies, width, label='MinMax Scaling', alpha=0.8)\n",
    "    plt.bar(x + width/2, standard_accuracies, width, label='Standard Scaling', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance Comparison: MinMax vs Standard Scaling')\n",
    "    plt.xticks(x, model_names, rotation=45, ha='right')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_scaling_comparison(results_minmax, results_standard, dl_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting the Best Performing Model\n",
    "def highlight_best_model(results_standard, dl_accuracy):\n",
    "    \"\"\"Highlight the best performing model based on standard scaling\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    model_names = list(results_standard.keys())\n",
    "    standard_accuracies = [results_standard[name]['accuracy'] for name in model_names]\n",
    "\n",
    "    # Add Deep Learning\n",
    "    model_names.append('Deep Learning')\n",
    "    standard_accuracies.append(dl_accuracy)\n",
    "\n",
    "    # Identify best model\n",
    "    best_idx = np.argmax(standard_accuracies)\n",
    "    best_model = model_names[best_idx]\n",
    "    best_accuracy = standard_accuracies[best_idx]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if i == best_idx else 'skyblue' for i in range(len(model_names))]\n",
    "\n",
    "    plt.bar(model_names, standard_accuracies, color=colors)\n",
    "    plt.title(f'Best Model: {best_model} (Accuracy: {best_accuracy:.4f})')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, best_accuracy\n",
    "\n",
    "best_model_name, best_accuracy = highlight_best_model(results_standard, dl_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f773577-e705-4550-a88a-41797c0d6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def plot_confusion_matrix_only(y_test, y_pred, model_name, class_labels):\n",
    "    \"\"\"Display only the confusion matrix graph.\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_labels,\n",
    "                yticklabels=class_labels)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix_only(y_test_std, results_standard[best_model_name]['predictions'], best_model_name, target_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1560fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays only the top 10 feature importances (for tree-based models)\n",
    "def plot_feature_importance(model, model_name):\n",
    "    \"\"\"Display only the feature importance graph for tree-based models.\"\"\"\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importance - {model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    plot_feature_importance(results_standard[best_model_name]['model'], best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb2e76-eba4-42b2-b061-0d5ecdc9009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis\n",
    "def cross_validation_analysis(results_standard, X_train_std, y_train_std):\n",
    "    \"\"\"Perform cross-validation analysis for model stability\"\"\"\n",
    "    \n",
    "    print(\"Cross-Validation Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, result in results_standard.items():\n",
    "        model = result['model']\n",
    "        cv_scores = cross_val_score(model, X_train_std, y_train_std, cv=5, scoring='accuracy')\n",
    "        \n",
    "        cv_results[name] = {\n",
    "            'mean': cv_scores.mean(),\n",
    "            'std': cv_scores.std(),\n",
    "            'scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(f\"  Individual CV Scores: {cv_scores}\")\n",
    "        print()\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    model_names = list(cv_results.keys())\n",
    "    means = [cv_results[name]['mean'] for name in model_names]\n",
    "    stds = [cv_results[name]['std'] for name in model_names]\n",
    "    \n",
    "    plt.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    plt.title('Cross-Validation Results (5-Fold)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Models')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "cv_results = cross_validation_analysis(results_standard, X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3305fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95120a-2289-4584-b888-5a15b7bf7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Improved Prediction Function - Clean and Concise\n",
    "def predict_salary_clean(new_data, model_name, results_standard, scaler_standard, label_encoders, target_encoder):\n",
    "    \"\"\"Clean function to predict salary for new data\"\"\"\n",
    "    \n",
    "    print(f\"Making predictions using {model_name}...\")\n",
    "    \n",
    "    # Create a copy and preprocess\n",
    "    processed_data = new_data.copy()\n",
    "    \n",
    "    # Handle education-num column name\n",
    "    if 'education-num' in processed_data.columns:\n",
    "        processed_data['educational-num'] = processed_data['education-num']\n",
    "        processed_data.drop('education-num', axis=1, inplace=True)\n",
    "    \n",
    "    # Feature engineering\n",
    "    processed_data['age_group'] = pd.cut(processed_data['age'], \n",
    "                                       bins=[0, 25, 35, 50, 65, 100], \n",
    "                                       labels=['Young', 'Adult', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "    \n",
    "    processed_data['hours_category'] = pd.cut(processed_data['hours-per-week'], \n",
    "                                            bins=[0, 20, 40, 60, 100], \n",
    "                                            labels=['Part-time', 'Full-time', 'Overtime', 'Workaholic'])\n",
    "    \n",
    "    processed_data['capital_net'] = processed_data['capital-gain'] - processed_data['capital-loss']\n",
    "    processed_data['has_capital_gain'] = (processed_data['capital-gain'] > 0).astype(int)\n",
    "    processed_data['has_capital_loss'] = (processed_data['capital-loss'] > 0).astype(int)\n",
    "    \n",
    "    # Education mapping\n",
    "    education_mapping = {\n",
    "        'Doctorate': 'Advanced', 'Prof-school': 'Advanced', 'Masters': 'Advanced',\n",
    "        'Bachelors': 'Bachelors', 'Some-college': 'Some-college',\n",
    "        'Assoc-acdm': 'Associate', 'Assoc-voc': 'Associate',\n",
    "        'HS-grad': 'High-school', '12th': 'High-school', '11th': 'High-school',\n",
    "        '10th': 'High-school', '9th': 'High-school', '7th-8th': 'Elementary'\n",
    "    }\n",
    "    \n",
    "    education_num_mapping = {\n",
    "        16: 'Doctorate', 15: 'Prof-school', 14: 'Masters', 13: 'Bachelors',\n",
    "        12: 'Some-college', 11: 'Assoc-acdm', 10: 'Assoc-voc', 9: 'HS-grad',\n",
    "        8: '12th', 7: '11th', 6: '10th', 5: '9th', 4: '7th-8th'\n",
    "    }\n",
    "    \n",
    "    if 'educational-num' in processed_data.columns:\n",
    "        processed_data['education'] = processed_data['educational-num'].map(education_num_mapping).fillna('HS-grad')\n",
    "        processed_data['education_grouped'] = processed_data['education'].map(education_mapping).fillna('Other')\n",
    "\n",
    "    # Define expected feature order\n",
    "    training_columns = [\n",
    "        'age', 'workclass', 'educational-num', 'marital-status', 'occupation',\n",
    "        'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',\n",
    "        'hours-per-week', 'native-country', 'age_group', 'hours_category',\n",
    "        'capital_net', 'has_capital_gain', 'has_capital_loss', 'education_grouped'\n",
    "    ]\n",
    "    \n",
    "    # Create final dataframe with correct order\n",
    "    final_data = pd.DataFrame()\n",
    "    for col in training_columns:\n",
    "        if col in processed_data.columns:\n",
    "            final_data[col] = processed_data[col]\n",
    "        else:\n",
    "            # Add default values for missing columns\n",
    "            final_data[col] = 0 if col in ['has_capital_gain', 'has_capital_loss', 'capital_net'] else processed_data.iloc[0, 0]\n",
    "    \n",
    "    # Label encoding with error handling\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in final_data.columns:\n",
    "            try:\n",
    "                final_data[col] = encoder.transform(final_data[col])\n",
    "            except ValueError:\n",
    "                # Use first class as fallback for unseen labels\n",
    "                final_data[col] = 0\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_new_data = scaler_standard.transform(final_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    if model_name == 'Deep Learning':\n",
    "        prediction_proba = dl_model.predict(scaled_new_data)\n",
    "        prediction = (prediction_proba > 0.5).astype(int).flatten()\n",
    "    else:\n",
    "        model = results_standard[model_name]['model']\n",
    "        prediction = model.predict(scaled_new_data)\n",
    "    \n",
    "    # Decode prediction\n",
    "    prediction_decoded = target_encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return prediction_decoded\n",
    "\n",
    "# Test with new sample\n",
    "new_sample = pd.DataFrame({\n",
    "    'age': [27], 'workclass': ['Private'], 'education-num': [12], 'marital-status': ['Married-civ-spouse'],\n",
    "    'occupation': ['Tech-support'], 'relationship': ['Wife'], 'race': ['White'], 'gender': ['Female'],\n",
    "    'capital-gain': [0], 'capital-loss': [0], 'hours-per-week': [38],\n",
    "    'native-country': ['United-States']\n",
    "})\n",
    "\n",
    "# Clean prediction call\n",
    "try:\n",
    "    prediction = predict_salary_clean(new_sample, best_model_name, results_standard, scaler_standard, label_encoders, target_encoder)\n",
    "    print(f\"Predicted salary category: {prediction[0]}\")\n",
    "    print(f\"Confidence: {best_accuracy:.1%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in clean prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more robust version with better error handling\n",
    "def predict_salary_robust(new_data, model_name=\"Gradient Boosting\"):\n",
    "    \"\"\"Most robust prediction function with comprehensive error handling\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Quick preprocessing\n",
    "        data = new_data.copy()\n",
    "        \n",
    "        # Handle education-num column name\n",
    "        if 'education-num' in data.columns:\n",
    "            data['educational-num'] = data['education-num']\n",
    "            data.drop('education-num', axis=1, inplace=True)\n",
    "        elif 'educational-num' not in data.columns:\n",
    "            data['educational-num'] = 9  # Default to HS-grad equivalent\n",
    "        \n",
    "        # Add required features with safe defaults\n",
    "        data['age_group'] = pd.cut(data['age'], bins=[0, 25, 35, 50, 65, 100], \n",
    "                                  labels=['Young', 'Adult', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "        data['hours_category'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], \n",
    "                                       labels=['Part-time', 'Full-time', 'Overtime', 'Workaholic'])\n",
    "        data['capital_net'] = data['capital-gain'] - data['capital-loss']\n",
    "        data['has_capital_gain'] = (data['capital-gain'] > 0).astype(int)\n",
    "        data['has_capital_loss'] = (data['capital-loss'] > 0).astype(int)\n",
    "        data['education_grouped'] = 'High-school'\n",
    "        \n",
    "        # Define training columns\n",
    "        training_columns = [\n",
    "            'age', 'workclass', 'educational-num', 'marital-status', 'occupation',\n",
    "            'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',\n",
    "            'hours-per-week', 'native-country', 'age_group', 'hours_category',\n",
    "            'capital_net', 'has_capital_gain', 'has_capital_loss', 'education_grouped'\n",
    "        ]\n",
    "        \n",
    "        # Create final dataframe with correct order and safe defaults\n",
    "        final_data = pd.DataFrame()\n",
    "        for col in training_columns:\n",
    "            if col in data.columns:\n",
    "                final_data[col] = data[col]\n",
    "            else:\n",
    "                # Safe default values\n",
    "                if col in ['has_capital_gain', 'has_capital_loss', 'capital_net']:\n",
    "                    final_data[col] = 0\n",
    "                elif col == 'age_group':\n",
    "                    final_data[col] = 'Adult'\n",
    "                elif col == 'hours_category':\n",
    "                    final_data[col] = 'Full-time'\n",
    "                elif col == 'education_grouped':\n",
    "                    final_data[col] = 'High-school'\n",
    "                else:\n",
    "                    # Use the first available value or a default\n",
    "                    final_data[col] = data.iloc[0, 0] if len(data.columns) > 0 else 0\n",
    "        \n",
    "        # Encode with comprehensive error handling\n",
    "        for col, encoder in label_encoders.items():\n",
    "            if col in final_data.columns:\n",
    "                try:\n",
    "                    final_data[col] = encoder.transform(final_data[col])\n",
    "                except:\n",
    "                    # Use mode of training data or 0 as fallback\n",
    "                    final_data[col] = 0\n",
    "        \n",
    "        # Scale and predict\n",
    "        scaled_data = scaler_standard.transform(final_data)\n",
    "        prediction = results_standard[model_name]['model'].predict(scaled_data)\n",
    "        \n",
    "        return target_encoder.inverse_transform(prediction)[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        return \"Unable to predict - please check input data format\"\n",
    "\n",
    "# Test robust function\n",
    "print(\"\\n=== Testing Robust Prediction Function ===\")\n",
    "robust_result = predict_salary_robust(new_sample)\n",
    "print(f\"Robust prediction result: {robust_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Even more concise version\n",
    "def predict_salary_simple(new_data, model_name=\"Gradient Boosting\"):\n",
    "    \"\"\"Ultra-simple prediction function\"\"\"\n",
    "    \n",
    "    # Quick preprocessing\n",
    "    data = new_data.copy()\n",
    "    \n",
    "    # Handle education-num column name\n",
    "    if 'education-num' in data.columns:\n",
    "        data['educational-num'] = data['education-num']\n",
    "        data.drop('education-num', axis=1, inplace=True)\n",
    "    \n",
    "    # Add required features\n",
    "    data['age_group'] = pd.cut(data['age'], bins=[0, 25, 35, 50, 65, 100], \n",
    "                              labels=['Young', 'Adult', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "    data['hours_category'] = pd.cut(data['hours-per-week'], bins=[0, 20, 40, 60, 100], \n",
    "                                   labels=['Part-time', 'Full-time', 'Overtime', 'Workaholic'])\n",
    "    data['capital_net'] = data['capital-gain'] - data['capital-loss']\n",
    "    data['has_capital_gain'] = (data['capital-gain'] > 0).astype(int)\n",
    "    data['has_capital_loss'] = (data['capital-loss'] > 0).astype(int)\n",
    "    data['education_grouped'] = 'High-school'  # Default value\n",
    "    \n",
    "    # Define training columns (moved inside function)\n",
    "    training_columns = [\n",
    "        'age', 'workclass', 'educational-num', 'marital-status', 'occupation',\n",
    "        'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',\n",
    "        'hours-per-week', 'native-country', 'age_group', 'hours_category',\n",
    "        'capital_net', 'has_capital_gain', 'has_capital_loss', 'education_grouped'\n",
    "    ]\n",
    "    \n",
    "    # Create final dataframe with correct order\n",
    "    final_data = pd.DataFrame()\n",
    "    for col in training_columns:\n",
    "        if col in data.columns:\n",
    "            final_data[col] = data[col]\n",
    "        else:\n",
    "            # Add default values for missing columns\n",
    "            final_data[col] = 0 if col in ['has_capital_gain', 'has_capital_loss', 'capital_net'] else data.iloc[0, 0]\n",
    "    \n",
    "    # Encode and predict\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in final_data.columns:\n",
    "            try:\n",
    "                final_data[col] = encoder.transform(final_data[col])\n",
    "            except:\n",
    "                final_data[col] = 0\n",
    "    \n",
    "    # Scale and predict\n",
    "    scaled_data = scaler_standard.transform(final_data)\n",
    "    prediction = results_standard[model_name]['model'].predict(scaled_data)\n",
    "    \n",
    "    return target_encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "# Usage example for simple function\n",
    "try:\n",
    "    result = predict_salary_simple(new_sample)\n",
    "    print(f\"\\nSimple prediction result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in simple prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7d712-f8d5-4286-9b04-dcd6612d27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Model Optimization Summary\n",
    "def optimization_summary():\n",
    "    \"\"\"Summarize all optimization techniques used\"\"\"\n",
    "    \n",
    "    print(\"=== MODEL OPTIMIZATION SUMMARY ===\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. DATA PREPROCESSING:\")\n",
    "    print(\"   - Handled missing values (replaced '?' with mode)\")\n",
    "    print(\"   - Removed inconsistent entries (Never-worked, Without-pay)\")\n",
    "    print(\"   - Filtered out very low education levels\")\n",
    "    print(\"   - Outlier detection and removal using IQR method\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. FEATURE ENGINEERING:\")\n",
    "    print(\"   - Created age groups and work hour categories\")\n",
    "    print(\"   - Generated capital net feature (gain - loss)\")\n",
    "    print(\"   - Added binary features for capital gain/loss\")\n",
    "    print(\"   - Grouped education levels for better representation\")\n",
    "    print(\"   - Removed redundant features (fnlwgt)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. NORMALIZATION TECHNIQUES:\")\n",
    "    print(\"   - MinMax Scaling (0-1 range)\")\n",
    "    print(\"   - Standard Scaling (mean=0, std=1)\")\n",
    "    print(\"   - Compared performance of both methods\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. HYPERPARAMETER OPTIMIZATION:\")\n",
    "    print(\"   - Grid Search CV for all traditional ML models\")\n",
    "    print(\"   - 5-fold cross-validation for model selection\")\n",
    "    print(\"   - Early stopping for deep learning model\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. DEEP LEARNING OPTIMIZATIONS:\")\n",
    "    print(\"   - Batch normalization for stable training\")\n",
    "    print(\"   - Dropout layers for regularization\")\n",
    "    print(\"   - Adam optimizer with learning rate scheduling\")\n",
    "    print(\"   - Multiple hidden layers with decreasing units\")\n",
    "    print()\n",
    "    \n",
    "    print(\"6. MODEL EVALUATION:\")\n",
    "    print(\"   - Stratified train-test split\")\n",
    "    print(\"   - Cross-validation analysis\")\n",
    "    print(\"   - Confusion matrix and classification reports\")\n",
    "    print(\"   - Feature importance analysis\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"BEST MODEL: {best_model_name}\")\n",
    "    print(f\"BEST ACCURACY: {best_accuracy:.4f}\")\n",
    "\n",
    "optimization_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMPLOYEE SALARY PREDICTION ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
